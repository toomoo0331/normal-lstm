{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Reshape, BatchNormalization, Add\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.network import Network\n",
    "from keras import backend as K\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "from keras.initializers import glorot_normal,orthogonal\n",
    "\n",
    "from keras.callbacks import EarlyStopping,TensorBoard\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# sess = tf.Session(config=config)\n",
    "# K.set_session(sess)\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#どんくらいで学習させるか関連\n",
    "timesteps=10 #一回のLSTMに入れる値の数\n",
    "camera=5 #カメラの数\n",
    "\n",
    "rate = 5 #飛ばすフレーム数（30FPSを30/rateのFPSの動画に疑似変換する．）\n",
    "max_data = 7000 #AUG前の学習データの最大値\n",
    "\n",
    "#画像関連 \n",
    "channels=3\n",
    "img_width=50\n",
    "img_height=50\n",
    "\n",
    "#LSTMなどで用いる値\n",
    "epochs = 100 # エポック数 \n",
    "batch_size = 5 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, camera_num, file_name, timesteps, width, height,max_data):\n",
    "        self.camera_num = camera_num\n",
    "        self.file_name =file_name\n",
    "        self.samples=0\n",
    "        self.timesteps=timesteps\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.max_data=max_data\n",
    "        self.save_numpy()\n",
    "        \n",
    "    def save_numpy(self):\n",
    "        self.samples = int(self.get_sample()/rate)\n",
    "        if not (os.path.isfile(self.file_name+\"label_\"+str(rate)+\".npy\")):\n",
    "            self.make_numpy_learnData()\n",
    "            self.make_numpy_labelData()\n",
    "        self.learnData=np.load(self.file_name+\"learn_\"+str(rate)+\".npy\")\n",
    "        self.labelData=np.load(self.file_name+\"label_\"+str(rate)+\".npy\")\n",
    "    \n",
    "    def get_sample(self):\n",
    "        video_path = self.file_name+\"video/0.mp4\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return num\n",
    "    \n",
    "    def make_numpy_labelData(self):\n",
    "        labelData=[]\n",
    "        test=[0,0,0,0,0]\n",
    "        csv_selection=csv.reader(open(self.file_name+\"video/mintime_optimize.csv\", 'r'))\n",
    "        one_hot = np.eye(self.camera_num)\n",
    "        for i,row2 in enumerate(csv_selection):\n",
    "            if(i%rate==0 and i > (self.timesteps-1)*rate-1 and i < self.max_data):\n",
    "                labelData.append(one_hot[int(row2[1])])\n",
    "                test[int(row2[1])]+=1\n",
    "        tmp_list = np.array(labelData)\n",
    "        tmp_list = tmp_list.astype(np.float)\n",
    "        np.save(self.file_name+\"label_\"+str(rate)+\".npy\", tmp_list)\n",
    "        print(tmp_list.shape)\n",
    "        print(test)\n",
    "    \n",
    "    \n",
    "    def make_numpy_learnData(self):\n",
    "        learnData=[]\n",
    "        for i in range(0, self.camera_num):\n",
    "            print()\n",
    "            img_list=[]\n",
    "            video_path = self.file_name+\"video/\"+str(i)+\".mp4\"\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if (video_len>self.max_data):\n",
    "                video_len=self.max_data\n",
    "            for j in range(0,video_len):\n",
    "                \n",
    "                pro_size=20\n",
    "                bar = int(j*pro_size/video_len)\n",
    "                pro_bar = ('=' * bar) + (' ' * (pro_size - bar))\n",
    "                percent ='{:03f}'.format(j / video_len * 100.)\n",
    "                print('\\r{0}/{1} [{2}] {3}%'.format((i+1), self.camera_num, pro_bar, percent), end='')\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if(j%rate==0):\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    img=Image.fromarray(frame)\n",
    "                    img = img.resize((self.width, self.height))\n",
    "                    x = np.array(img, dtype=np.float32)\n",
    "                    x = x / 255.\n",
    "                    img_list.append(x)\n",
    "            learnData.append(img_list)\n",
    "            \n",
    "        tmp_list=np.array(learnData)\n",
    "        tmp_list=tmp_list.astype(np.float)\n",
    "        np.save(self.file_name+\"learn_\"+str(rate)+\".npy\", tmp_list)\n",
    "        print(tmp_list.shape)\n",
    "    \n",
    "    \n",
    "    def make_learndata(self, num=[]):\n",
    "        data_name=self.file_name+\"learn_0_\"+str(rate)+\".npy\"\n",
    "        if (os.path.isfile(data_name)):\n",
    "            test=np.load(data_name)\n",
    "        else:\n",
    "            all_list = [np.empty((0,self.timesteps, self.width, self.height, 3), np.float)]*self.camera_num\n",
    "            tmp_list = [np.empty((0,self.timesteps, self.width, self.height, 3), np.float)]*self.camera_num\n",
    "            for i in range(0, self.labelData.shape[0]):\n",
    "                #まずはすべての分だけ回す\n",
    "                pro_size=20\n",
    "                bar = int(i*pro_size/self.labelData.shape[0])\n",
    "                pro_bar = ('=' * bar) + (' ' * (pro_size - bar))\n",
    "                percent ='{:03f}'.format(i / self.labelData.shape[0] * 100.)\n",
    "                print('\\r [{0}] {1}%'.format(pro_bar, percent), end='')\n",
    "                \n",
    "                count=num[np.argmax(self.labelData[i,:])]\n",
    "                \n",
    "                for j in range(0,count):\n",
    "                    #dataAugumentationする分だけ回す\n",
    "                    #dataAugmentationは異なるカメラでも同じ時間軸の場合同じ角度で回したいため，はじめに角度を取得する．\n",
    "                    random_angle = float(random.randint(0,180))\n",
    "                    \n",
    "                    for camera in range(0, self.camera_num):\n",
    "                        #カメラの台数だけ回す\n",
    "                        #tmp_list[camera]の同じ引数（番号）には角度を同じ分だけずらした同じ時間軸のsequenceが入っている．\n",
    "                        first_list =np.empty((0, self.width, self.height, 3), np.float)\n",
    "                        for l in range(0, self.timesteps):\n",
    "                            #timesteps分のndarrayを作る．\n",
    "                            tmp_img=self.learnData[camera,i+l]\n",
    "                            img = self.augumentation(tmp_img, random_angle)\n",
    "                            first_list =np.append(first_list, [img], axis=0)\n",
    "                        #あるカメラについて，１つのsequenceがfirst_listに出来上がっている状態．\n",
    "                        tmp_list[camera]=np.append(tmp_list[camera], [first_list], axis=0)\n",
    "                if(tmp_list[0].shape[0]>100 or i == self.labelData.shape[0]-1):\n",
    "                    #処理時間短縮のため，sequenceがtmp_listに溜まってきたら，全体に統合してまたtmp_listを初期化する．\n",
    "                    for camera in range(0, self.camera_num):\n",
    "                        all_list[camera]=np.append(all_list[camera], tmp_list[camera], axis=0)\n",
    "                        tmp_list[camera] = np.empty((0,self.timesteps, self.width, self.height, 3), np.float)\n",
    "            \n",
    "            for camera in range(0, self.camera_num):\n",
    "                print(all_list[camera].shape)\n",
    "                data_name=self.file_name+\"learn_\"+str(camera)+\"_\"+str(rate)\n",
    "                np.save(data_name, all_list[camera])\n",
    "    \n",
    "    \n",
    "    def make_labeldata(self, num=[]):\n",
    "        data_name=self.file_name+\"final_label_\"+str(rate)+\"_.npy\"\n",
    "        if (os.path.isfile(data_name)):\n",
    "            test=np.load(data_name)\n",
    "        else:\n",
    "            test = np.empty((0,5), np.float)\n",
    "            for i in range(0,self.labelData.shape[0]):\n",
    "                count=num[np.argmax(self.labelData[i,:])]\n",
    "                for j in range(0, count):\n",
    "                    test = np.append(test, [self.labelData[i]], axis=0)\n",
    "            print(test.shape)\n",
    "            np.save(data_name, test)\n",
    "    \n",
    "    \n",
    "    def get_learndata(self, num):\n",
    "        data_name=self.file_name+\"learn_\"+str(num)+\"_\"+str(rate)+\".npy\"\n",
    "        test=np.load(data_name)\n",
    "        return test\n",
    "    \n",
    "    \n",
    "    def get_labeldata(self):\n",
    "        data_name=self.file_name+\"final_label_\"+str(rate)+\"_.npy\"\n",
    "        test=np.load(data_name)\n",
    "        return test\n",
    "    \n",
    "    \n",
    "    def augumentation(self, img, num):\n",
    "        height = img.shape[0]                \n",
    "        width = img.shape[1]                       \n",
    "        center = (int(width/2), int(height/2))\n",
    "        angle = num\n",
    "        scale = 1.0\n",
    "        trans = cv2.getRotationMatrix2D(center, angle , scale)\n",
    "        image2 = cv2.warpAffine(img, trans, (width,height))\n",
    "        return image2\n",
    "        \n",
    "    \n",
    "    def show(self):\n",
    "        print(0,self.get_learndata(0).shape)\n",
    "        print(1,self.get_learndata(1).shape)\n",
    "        print(2,self.get_learndata(2).shape)\n",
    "        print(3,self.get_learndata(3).shape)\n",
    "        print(4,self.get_learndata(4).shape)\n",
    "        print(self.get_labeldata().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(camera, \"./data/20190427/\", timesteps, img_width, img_height,max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_rate=[2,25,8,3,1]\n",
    "data.make_learndata(aug_rate)\n",
    "data.make_labeldata(aug_rate)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add for TensorBoard\n",
    "\n",
    "old_session = KTF.get_session()\n",
    "session = tf.Session('')\n",
    "KTF.set_session(session)\n",
    "KTF.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18 :\n",
    "    ##\n",
    "    ##　Resnet-18の作成\n",
    "    ##\n",
    "    def __init__(self, width, height, channels):\n",
    "        self.img_rows=height\n",
    "        self.img_cols=width\n",
    "        self.img_channels=channels\n",
    "    \n",
    "    def rescell(self, data, filters, kernel_size, option=False):\n",
    "        strides=(1,1)\n",
    "        if option:\n",
    "            strides=(2,2)\n",
    "        x=Convolution2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(data)\n",
    "        x=BatchNormalization()(x)\n",
    "        x=Activation('relu')(x)\n",
    "        \n",
    "        data=Convolution2D(filters=int(x.shape[3]), kernel_size=(1,1), strides=strides, padding=\"same\")(data)\n",
    "        x=Convolution2D(filters=filters,kernel_size=kernel_size,strides=(1,1),padding=\"same\")(x)\n",
    "        x=BatchNormalization()(x)\n",
    "        x=Add()([x,data])\n",
    "        x=Activation('relu')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def create_resnet18(self):\n",
    "        inputs=Input(shape=(self.img_rows, self.img_cols, self.img_channels))\n",
    "        x=Convolution2D(64,(7,7), padding=\"same\", input_shape=(self.img_rows, self.img_cols),activation=\"relu\")(inputs)\n",
    "        x=MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "        x=self.rescell(x,64,(3,3))\n",
    "        x=self.rescell(x,64,(3,3))\n",
    "        x=self.rescell(x,64,(3,3))\n",
    "        \n",
    "        x=self.rescell(x,128,(3,3),True)\n",
    "        x=self.rescell(x,128,(3,3))\n",
    "\n",
    "        x=self.rescell(x,256,(3,3),True)\n",
    "        x=self.rescell(x,256,(3,3))\n",
    "        \n",
    "        x=self.rescell(x,512,(3,3),True)\n",
    "        x=self.rescell(x,512,(3,3))\n",
    "        \n",
    "        model=Model(inputs=inputs,outputs=[x])\n",
    "        return model\n",
    "\n",
    "    def get_resnet18(self):\n",
    "        return self.create_resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction :\n",
    "    def __init__(self, timesteps, n_out, width, height):\n",
    "        self.maxlen = timesteps\n",
    "        self.n_out = n_out\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.resnet = Resnet18(width, height, 3).get_resnet18()\n",
    "        self.sharedLayer = self.create_sharedmodel()\n",
    "    \n",
    "    \n",
    "    def create_sharedmodel(self):\n",
    "        inputs = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        \n",
    "        x = TimeDistributed(self.resnet,name=\"resnet\")(inputs)\n",
    "        x = TimeDistributed(GlobalAveragePooling2D(), name=\"GAP\")(x)\n",
    "        x = TimeDistributed(Dense(512,activation='relu'), name=\"dense\")(x)\n",
    "        predictions = LSTM(256, batch_input_shape = (None, self.maxlen, 512),\n",
    "             kernel_initializer = glorot_normal(seed=20181020),\n",
    "             recurrent_initializer = orthogonal(gain=1.0, seed=20181020), \n",
    "             dropout = 0.01, \n",
    "             recurrent_dropout = 0.01)(x)\n",
    "        \n",
    "        shared_layers = Model(inputs, predictions, name=\"shared_layers\")\n",
    "        \n",
    "        return shared_layers\n",
    "    \n",
    "    \n",
    "    def create_model(self):\n",
    "        model_input1 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input2 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input3 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input4 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input5 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        \n",
    "        mid_feature1 = self.sharedLayer(model_input1)\n",
    "        mid_feature2 = self.sharedLayer(model_input2)\n",
    "        mid_feature3 = self.sharedLayer(model_input3)\n",
    "        mid_feature4 = self.sharedLayer(model_input4)\n",
    "        mid_feature5 = self.sharedLayer(model_input5)\n",
    "        \n",
    "        merged_vector = keras.layers.concatenate([mid_feature1, mid_feature2, mid_feature3, mid_feature4, mid_feature5], axis=-1)\n",
    "        mid_dense = Dense(256, activation='relu')(merged_vector)\n",
    "        predictions = Dense(self.n_out, activation='softmax')(mid_dense)\n",
    "        \n",
    "        model = Model(inputs=[model_input1, model_input2, model_input3, model_input4, model_input5], outputs=predictions)\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, t_train, x_vali, t_vali, batch_size, epochs) :\n",
    "        early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "        model = self.create_model()\n",
    "        hist = model.fit(x=[x_train[0], x_train[1], x_train[2], x_train[3], x_train[4]], y=t_train, batch_size = batch_size, epochs = epochs, validation_data=([x_vali[0], x_vali[1], x_vali[2], x_vali[3], x_vali[4]], t_vali), callbacks = [early_stopping])\n",
    "        return model, hist\n",
    "    \n",
    "    \n",
    "    def train_keras(self, x_train, t_train, batch_size, epochs) :\n",
    "        early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "        model = self.create_model()\n",
    "        hist = model.fit([x_train[0], x_train[1], x_train[2], x_train[3], x_train[4]], t_train, batch_size = batch_size, epochs = epochs, verbose = 1,\n",
    "                         callbacks = [early_stopping], validation_split = 0.2)\n",
    "        return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "pred = Prediction(timesteps, camera, img_width, img_height)\n",
    "model = pred.create_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "test_learn=[np.empty((data.get_labeldata().shape[0],10, 50, 50, 3), np.float)]*camera\n",
    "vali_learn=[np.empty((0,10, 50, 50, 3), np.float)]*camera\n",
    "test_label=data.get_labeldata()\n",
    "vali_label=np.empty((0,camera), np.float)\n",
    "\n",
    "print(test_label.shape)\n",
    "\n",
    "for j in range(0,camera):\n",
    "    test_learn[j]=data.get_learndata(j)\n",
    "\n",
    "# for i in tqdm(range(0,int(0.1*test_label.shape[0]))):\n",
    "#     random_angle = int(random.randint(0,test_label.shape[0]-1))\n",
    "#     for j in range(0,camera):\n",
    "#         vali_learn[j]=np.append(vali_learn[j], [test_learn[j][random_angle]],axis=0)\n",
    "#         test_learn[j]=np.delete(test_learn[j], random_angle,0)\n",
    "#     vali_label=np.append(vali_label, [test_label[random_angle]], axis=0)\n",
    "#     test_label=np.delete(test_label, random_angle,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "#model , hist = pred.train(test_learn, test_label, vali_learn, vali_label, batch_size, epochs)\n",
    "model , hist = pred.train_keras(test_learn, test_label, batch_size, epochs)\n",
    "# テスト\n",
    "score = model.evaluate(test_learn, test_label, batch_size = batch_size, verbose = 1)\n",
    "print(\"score:\", score)\n",
    "\n",
    "\n",
    "# # ### add for TensorBoard\n",
    "# # KTF.set_session(old_session)\n",
    "# # ###\n",
    "\n",
    "\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# #                            モデルのアーキテクチャの保存\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# # モデルのアーキテクチャの保存①(JSON版)\n",
    "# model_arc_json = model.to_json()\n",
    "# open(\"data/20190427/model_architecture.json\", mode='w').write(model_arc_json)\n",
    "\n",
    "\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# #                            　　モデルの重みの保存\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# # モデルの重みの保存\n",
    "# model.save_weights(\"data/20190427/weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_loss(hist):\n",
    "    # 損失値(Loss)の遷移のプロット\n",
    "    plt.plot(hist.history['loss'],label=\"loss for training\")\n",
    "    plt.plot(hist.history['val_loss'],label=\"loss for validation\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_history_acc(hist):\n",
    "    # 精度(Accuracy)の遷移のプロット\n",
    "    plt.plot(hist.history['acc'],label=\"loss for training\")\n",
    "    plt.plot(hist.history['val_acc'],label=\"loss for validation\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "\n",
    "plot_history_loss(hist)\n",
    "plot_history_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model):\n",
    "    for i in model.layers:\n",
    "        i.trainable = False\n",
    "        if isinstance(i, Model):\n",
    "            freeze_layers(i)\n",
    "    return model\n",
    "\n",
    "model_freezed = freeze_layers(model)\n",
    "model_freezed.save('file.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#試すやつ！\n",
    "\n",
    "pre = [[0 for i in range(5)] for j in range(5)]\n",
    "print(test_learn[0].shape)\n",
    "# 正答率集計\n",
    "preds = model.predict([test_learn[0], test_learn[1], test_learn[2], test_learn[3], test_learn[4]])\n",
    "correct = 0\n",
    "\n",
    "f = open(\"./data/20190427/result.csv\", 'w')\n",
    "writer = csv.writer(f)\n",
    "for i in range(0,timesteps):\n",
    "    writer.writerow([0])\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    pred_result = np.argmax(preds[i,:])\n",
    "    tar = np.argmax(data.get_labeldata()[i,:])\n",
    "    for j in range(0,rate):\n",
    "        writer.writerow([pred_result])\n",
    "    pre[pred_result][tar]+=1\n",
    "    print(pred_result, tar)\n",
    "    if pred_result == tar :\n",
    "        correct += 1\n",
    "\n",
    "print(\"正答率:\", 1.0 * correct / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
