{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Reshape, BatchNormalization, Add\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.network import Network\n",
    "from keras import backend as K\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "from keras.initializers import glorot_normal,orthogonal\n",
    "\n",
    "from keras.callbacks import EarlyStopping,TensorBoard\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# sess = tf.Session(config=config)\n",
    "# K.set_session(sess)\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#どんくらいで学習させるか関連\n",
    "timesteps=10 #一回のLSTMに入れる値の数\n",
    "camera=5 #カメラの数\n",
    "\n",
    "rate = 5 #飛ばすフレーム数（30FPSを30/rateのFPSの動画に疑似変換する．）\n",
    "\n",
    "#画像関連 \n",
    "channels=3\n",
    "img_width=50\n",
    "img_height=50\n",
    "\n",
    "#LSTMなどで用いる値\n",
    "epochs = 100 # エポック数 \n",
    "batch_size = 5 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, camera_num, file_name, timesteps, width, height):\n",
    "        self.camera_num = camera_num\n",
    "        self.file_name =file_name\n",
    "        self.samples=0\n",
    "        self.timesteps=timesteps\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.save_numpy()\n",
    "        \n",
    "    def save_numpy(self):\n",
    "        self.samples = int(self.get_sample()/rate)\n",
    "        if not (os.path.isfile(self.file_name+\"label_\"+str(rate)+\".npy\")):\n",
    "            self.make_numpy_learnData()\n",
    "            self.make_numpy_labelData()\n",
    "        self.learnData=np.load(self.file_name+\"learn_\"+str(rate)+\".npy\")\n",
    "        self.labelData=np.load(self.file_name+\"label_\"+str(rate)+\".npy\")\n",
    "    \n",
    "    def get_sample(self):\n",
    "        video_path = self.file_name+\"video/0.mp4\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return num\n",
    "    \n",
    "    def make_numpy_labelData(self):\n",
    "        labelData=[]\n",
    "        csv_selection=csv.reader(open(self.file_name+\"video/mintime_optimize.csv\", 'r'))\n",
    "        one_hot = np.eye(self.camera_num)\n",
    "        for i,row2 in enumerate(csv_selection):\n",
    "            if(i%rate==0 and i > (self.timesteps-1)*rate-1):\n",
    "                labelData.append(one_hot[int(row2[1])])\n",
    "        tmp_list = np.array(labelData)\n",
    "        tmp_list = tmp_list.astype(np.float)\n",
    "        np.save(self.file_name+\"label_\"+str(rate)+\".npy\", tmp_list)\n",
    "        print(tmp_list.shape)\n",
    "    \n",
    "    \n",
    "    def make_numpy_learnData(self):\n",
    "        learnData=[]\n",
    "        for i in range(0, self.camera_num):\n",
    "            print()\n",
    "            img_list=[]\n",
    "            video_path = self.file_name+\"video/\"+str(i)+\".mp4\"\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            for j in range(0,video_len):\n",
    "                \n",
    "                pro_size=20\n",
    "                bar = int(j*pro_size/video_len)\n",
    "                pro_bar = ('=' * bar) + (' ' * (pro_size - bar))\n",
    "                percent ='{:03f}'.format(j / video_len * 100.)\n",
    "                print('\\r{0}/{1} [{2}] {3}%'.format((i+1), self.camera_num, pro_bar, percent), end='')\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if(j%rate==0):\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    img=Image.fromarray(frame)\n",
    "                    img = img.resize((self.width, self.height))\n",
    "                    x = np.array(img, dtype=np.float32)\n",
    "                    x = x / 255.\n",
    "                    img_list.append(x)\n",
    "            learnData.append(img_list)\n",
    "            \n",
    "        tmp_list=np.array(learnData)\n",
    "        tmp_list=tmp_list.astype(np.float)\n",
    "        np.save(self.file_name+\"learn_\"+str(rate)+\".npy\", tmp_list)\n",
    "        print(tmp_list.shape)\n",
    "    \n",
    "    \n",
    "    def make_learndata(self, num=[]):\n",
    "        data_name=self.file_name+\"learn_0_\"+str(rate)+\".npy\"\n",
    "        if (os.path.isfile(data_name)):\n",
    "            test=np.load(data_name)\n",
    "        else:\n",
    "            all_list = [np.empty((0,self.timesteps, self.width, self.height, 3), np.float)]*self.camera_num\n",
    "            tmp_list = [np.empty((0,self.timesteps, self.width, self.height, 3), np.float)]*self.camera_num\n",
    "            for i in range(0, self.labelData.shape[0]):\n",
    "                #まずはすべての分だけ回す\n",
    "                pro_size=20\n",
    "                bar = int(i*pro_size/self.labelData.shape[0])\n",
    "                pro_bar = ('=' * bar) + (' ' * (pro_size - bar))\n",
    "                percent ='{:03f}'.format(i / self.labelData.shape[0] * 100.)\n",
    "                print('\\r [{0}] {1}%'.format(pro_bar, percent), end='')\n",
    "                \n",
    "                count=num[np.argmax(self.labelData[i,:])]\n",
    "                \n",
    "                for j in range(0,count):\n",
    "                    #dataAugumentationする分だけ回す\n",
    "                    #dataAugmentationは異なるカメラでも同じ時間軸の場合同じ角度で回したいため，はじめに角度を取得する．\n",
    "                    random_angle = float(random.randint(0,180))\n",
    "                    \n",
    "                    for camera in range(0, self.camera_num):\n",
    "                        #カメラの台数だけ回す\n",
    "                        #tmp_list[camera]の同じ引数（番号）には角度を同じ分だけずらした同じ時間軸のsequenceが入っている．\n",
    "                        first_list =np.empty((0, self.width, self.height, 3), np.float)\n",
    "                        for l in range(0, self.timesteps):\n",
    "                            #timesteps分のndarrayを作る．\n",
    "                            tmp_img=self.learnData[camera,i+l]\n",
    "                            img = self.augumentation(tmp_img, random_angle)\n",
    "                            first_list =np.append(first_list, [img], axis=0)\n",
    "                        #あるカメラについて，１つのsequenceがfirst_listに出来上がっている状態．\n",
    "                        tmp_list[camera]=np.append(tmp_list[camera], [first_list], axis=0)\n",
    "                if(tmp_list[0].shape[0]>100 or i == self.labelData.shape[0]-1):\n",
    "                    #処理時間短縮のため，sequenceがtmp_listに溜まってきたら，全体に統合してまたtmp_listを初期化する．\n",
    "                    for camera in range(0, self.camera_num):\n",
    "                        all_list[camera]=np.append(all_list[camera], tmp_list[camera], axis=0)\n",
    "                        tmp_list[camera] = np.empty((0,self.timesteps, self.width, self.height, 3), np.float)\n",
    "            \n",
    "            for camera in range(0, self.camera_num):\n",
    "                print(all_list[camera].shape)\n",
    "                data_name=self.file_name+\"learn_\"+str(camera)+\"_\"+str(rate)\n",
    "                np.save(data_name, all_list[camera])\n",
    "                del all_list[camera]\n",
    "    \n",
    "    \n",
    "    def make_labeldata(self, num=[]):\n",
    "        data_name=self.file_name+\"final_label_\"+str(rate)+\"_.npy\"\n",
    "        if (os.path.isfile(data_name)):\n",
    "            test=np.load(data_name)\n",
    "        else:\n",
    "            test = np.empty((0,5), np.float)\n",
    "            for i in range(0,self.labelData.shape[0]):\n",
    "                count=num[np.argmax(self.labelData[i,:])]\n",
    "                for j in range(0, count):\n",
    "                    test = np.append(test, [self.labelData[i]], axis=0)\n",
    "            print(test.shape)\n",
    "            np.save(data_name, test)\n",
    "    \n",
    "    \n",
    "    def get_learndata(self, num):\n",
    "        data_name=self.file_name+\"learn_\"+str(num)+\"_\"+str(rate)+\".npy\"\n",
    "        test=np.load(data_name)\n",
    "        return test\n",
    "    \n",
    "    def get_labeldata(self):\n",
    "        data_name=self.file_name+\"final_label_\"+str(rate)+\"_.npy\"\n",
    "        test=np.load(data_name)\n",
    "        return test\n",
    "    \n",
    "    \n",
    "    def augumentation(self, img, num):\n",
    "        height = img.shape[0]                \n",
    "        width = img.shape[1]                       \n",
    "        center = (int(width/2), int(height/2))\n",
    "        angle = num\n",
    "        scale = 1.0\n",
    "        trans = cv2.getRotationMatrix2D(center, angle , scale)\n",
    "        image2 = cv2.warpAffine(img, trans, (width,height))\n",
    "        return image2\n",
    "        \n",
    "    \n",
    "    def show(self):\n",
    "        print(0,self.get_learndata(0).shape)\n",
    "        print(1,self.get_learndata(1).shape)\n",
    "        print(2,self.get_learndata(2).shape)\n",
    "        print(3,self.get_learndata(3).shape)\n",
    "        print(4,self.get_learndata(4).shape)\n",
    "        print(self.get_labeldata().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(camera, \"./data/20190426/\", timesteps, img_width, img_height)\n",
    "aug_rate=[3,7,7,2,1]\n",
    "data.make_learndata(aug_rate)\n",
    "data.make_labeldata(aug_rate)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction :\n",
    "    def __init__(self, timesteps, n_out, width, height):\n",
    "        self.maxlen = timesteps\n",
    "        self.n_out = n_out\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.resnet = Resnet18(width, height, 3).get_resnet18()\n",
    "        self.sharedLayer = self.create_sharedmodel()\n",
    "    \n",
    "    \n",
    "    def create_sharedmodel(self):\n",
    "        inputs = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        \n",
    "        x = TimeDistributed(self.resnet,name=\"resnet\")(inputs)\n",
    "        x = TimeDistributed(GlobalAveragePooling2D(), name=\"GAP\")(x)\n",
    "        x = TimeDistributed(Dense(512,activation='relu'), name=\"dense\")(x)\n",
    "        predictions = LSTM(256, batch_input_shape = (None, self.maxlen, 512),\n",
    "             kernel_initializer = glorot_normal(seed=20181020),\n",
    "             recurrent_initializer = orthogonal(gain=1.0, seed=20181020), \n",
    "             dropout = 0.01, \n",
    "             recurrent_dropout = 0.01)(x)\n",
    "        \n",
    "        shared_layers = Model(inputs, predictions, name=\"shared_layers\")\n",
    "        \n",
    "        return shared_layers\n",
    "    \n",
    "    \n",
    "    def create_model(self):\n",
    "        model_input1 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input2 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input3 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input4 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        model_input5 = Input(shape=(self.maxlen, self.height, self.width, 3))\n",
    "        \n",
    "        mid_feature1 = self.sharedLayer(model_input1)\n",
    "        mid_feature2 = self.sharedLayer(model_input2)\n",
    "        mid_feature3 = self.sharedLayer(model_input3)\n",
    "        mid_feature4 = self.sharedLayer(model_input4)\n",
    "        mid_feature5 = self.sharedLayer(model_input5)\n",
    "        \n",
    "        merged_vector1 = keras.layers.concatenate([mid_feature1, mid_feature2, mid_feature3, mid_feature4, mid_feature5], axis=-1)\n",
    "        merged_vector = keras.layers.concatenate([mid_feature1, merged_vector1], axis=-1)\n",
    "        mid_dense = Dense(256, activation='relu')(merged_vector)\n",
    "        predictions = Dense(self.n_out, activation='softmax')(mid_dense)\n",
    "        \n",
    "        model = Model(inputs=[model_input1, model_input2, model_input3, model_input4, model_input5], outputs=predictions)\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, t_train, x_vali, t_vali, batch_size, epochs) :\n",
    "        early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "        tb_cb = TensorBoard(log_dir=\"./data/20190426/tflog/\", histogram_freq=1)\n",
    "        model = self.create_model()\n",
    "        hist = model.fit(x=[x_train[0], x_train[1], x_train[2], x_train[3], x_train[4]], y=t_train, batch_size = batch_size, epochs = epochs, validation_data=([x_vali[0], x_vali[1], x_vali[2], x_vali[3], x_vali[4]], t_vali), callbacks = [early_stopping])\n",
    "        return model, hist\n",
    "    \n",
    "    \n",
    "    def train_keras(self, x_train, t_train, batch_size, epochs) :\n",
    "        early_stopping = EarlyStopping(patience=1, verbose=1)\n",
    "        tb_cb = TensorBoard(log_dir=\"./data/20190426/tflog/\", histogram_freq=1, write_graph=True, write_grads=True)\n",
    "        model = self.create_model()\n",
    "        hist = model.fit([x_train[0], x_train[1], x_train[2], x_train[3], x_train[4]], t_train, batch_size = batch_size, epochs = epochs, verbose = 1,\n",
    "                         callbacks = [early_stopping, tb_cb], validation_split = 0.2)\n",
    "        return model, hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
